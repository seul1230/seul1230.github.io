---
layout: post
title:  "TIL | GAN - NLP Text Generation Model"
date:   2022-12-21 03:30:09 +0900
categories: Data_AI
tags: [TIL, GAN, NLP]
---
# [ NLP ] GAN : Generative Adversarial Nets

<!-- LeakGAN : Long Text Generation via Adversarial Training with Leaked Information -->

ë³¸ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ë¬¸ì¥ ìƒì„± ëª¨ë¸ì— ê´€í•œ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ê¸° ìœ„í•´ ê³µë¶€í•œ ë‚´ìš©ì„ ì •ë¦¬í•˜ì˜€ë‹¤. GANì— ëŒ€í•´ ì•Œì•„ë³´ì. 

### Generative Model ì˜ ëª©í‘œ
<p align='center'>
<img src='/assets/img/data_ai_img/GAN_goal.png' width="90%">
</p>
ì‹œê°„ì´ ì§€ë‚˜ë©´ì„œ ìƒì„± ëª¨ë¸ì´ ì›ë³¸ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ í•™ìŠµí•œë‹¤. 

í•™ìŠµì´ ì˜ ë˜ì—ˆë‹¤ë©´ í†µê³„ì ìœ¼ë¡œ í‰ê· ì ì¸ íŠ¹ì§•ì„ ê°€ì§€ëŠ” ë°ì´í„°ë¥¼ ì‰½ê²Œ ìƒì„±í•  ìˆ˜ ìˆë‹¤. ì´ë•Œ íŒë³„ ëª¨ë¸ì€ ë” ì´ìƒ ì§„ì§œ ì´ë¯¸ì§€ì™€ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ êµ¬ë¶„í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ë¶„í¬ê°€ 1/2 ë¡œ ìˆ˜ë ´í•œë‹¤.


## GANì´ë€
Generative Adversarial Networksì˜ ì•½ìë¡œ,
ì‹¤ì œë¡œ ì¡´ì¬í•˜ì§€ ì•Šì§€ë§Œ ìˆì„ë²•í•œ ì´ë¯¸ì§€ë‚˜ í…ìŠ¤íŠ¸ ë“± ì—¬ëŸ¬ ë°ì´í„°ë¥¼ ìƒì„±í•´ ë‚´ëŠ” ëª¨ë¸ì´ë‹¤.

<p align='center'>
<img src='/assets/img/data_ai_img/GAN.gif' width="90%">
</p>

ì´ë¦„ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ GANì€ **ì„œë¡œ ë‹¤ë¥¸ ë‘ ê°œì˜ ë„¤íŠ¸ì›Œí¬**ë¥¼ ì ëŒ€ì ìœ¼ë¡œ(adversarial) í•™ìŠµì‹œí‚¤ë©° ì‹¤ì œ ë°ì´í„°ì™€ ë¹„ìŠ·í•œ ë°ì´í„°ë¥¼ ìƒì„±(generative)í•´ë‚´ëŠ” ëª¨ë¸ì´ë©° ì´ë ‡ê²Œ ìƒì„±ëœ ë°ì´í„°ì— ì •í•´ì§„ labelê°’ì´ ì—†ê¸° ë•Œë¬¸ì— ë¹„ì§€ë„ í•™ìŠµ ê¸°ë°˜ ìƒì„±ëª¨ë¸ë¡œ ë¶„ë¥˜ëœë‹¤.

ì´ë•Œ ë‘ ê°œì˜ ë„¤íŠ¸ì›Œí¬ëŠ” **ìƒì„±ì <font color='lightgray'>generator</font>** ì™€ **íŒë³„ì <font color='lightgray'>discriminator</font>** ë¥¼ ë§í•œë‹¤. í•™ìŠµì´ ë‹¤ ëœ ì´í›„ì—, ëª¨ë¸ì€ ìƒì„±ìë¼ê³  í•  ìˆ˜ ìˆìœ¼ë©° íŒë³„ìëŠ” ì´ ìƒì„±ìê°€ ì˜ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ê¸° ìœ„í•œ ëª©ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©í•œë‹¤. ì´ ë‘ ê°œì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ í•¨ê»˜ í•™ìŠµì‹œí‚¤ë©´ì„œ ê²°ê³¼ì ìœ¼ë¡œ ìƒì„± ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ëœë‹¤ !

<br/>

**[ ëª©ì  í•¨ìˆ˜ ]**

<p align='center'>
<img src='/assets/img/data_ai_img/objective_function.png' width="90%">
</p>

**ìƒì„±ì G** ëŠ” **í•¨ìˆ˜ V** ì˜ ê°’ì„ ë‚®ì¶”ê³ ì ë…¸ë ¥í•˜ê³  **íŒë³„ì D** ëŠ” ë†’ì´ë ¤ê³  ë…¸ë ¥í•œë‹¤. ì´ í•¨ìˆ˜ë¥¼ í†µí•´ ìƒì„±ìëŠ” ì´ë¯¸ì§€ ë¶„í¬ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

**1ï¸âƒ£ ì´ë¯¸ì§€**<br/>
ë°ì´í„°ì—ì„œ ì—¬ëŸ¬ ê°œì˜ ë°ì´í„° (ì´ë¯¸ì§€) ë¥¼ ë½‘ì•„ì™€ì„œ log ë³€í™˜í•´ì¤€ í›„ì˜ ê¸°ëŒ“ê°’, ì¦‰ í‰ê· ê°’ <br/>

**2ï¸âƒ£ ë…¸ì´ì¦ˆ**<br/>
ë…¸ì´ì¦ˆë¥¼ ìƒ˜í”Œë§í•´ì™€ì„œ ìƒì„±ì Gì— ë„£ì–´ì„œ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ë§Œë“  í›„ íŒë³„ì Dì— ë„£ì€ ê°’ì„ 1ì—ì„œ ë¹¼ì„œ Logë¥¼ ì·¨í•œ ê°’ì˜ í‰ê· ê°’  <br/><font color='lightgray'>ìƒì„±ìì— ëŒ€í•œ ê°œë…ì´ í¬í•¨. ê¸°ë³¸ì ìœ¼ë¡œ ìƒì„±ìëŠ” ë…¸ì´ì¦ˆ ë²¡í„°ë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ë°›ì•„ì™€ì„œ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ì§„ì§œ ì´ë¯¸ì§€ëŠ” 1, ê°€ì§œ ì´ë¯¸ì§€ëŠ” 0.</font>

### ëª¨ë¸ í•™ìŠµ
<p align='center'>
<img src='/assets/img/data_ai_img/GAN_epoch.png' width="90%">
</p>

ë§¤ epochë‹¹ **Descriminator**ë¥¼ ë¨¼ì € í•™ìŠµí•˜ê³ , **Generator**ì˜ í•™ìŠµì´ ì´ë£¨ì–´ì§„ë‹¤. <br/>
DescriminatorëŠ” ê¸°ìš¸ê¸° (Stochastic gradient) ê°€ **ì¦ê°€**í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµë˜ê³ , generatorëŠ” ê¸°ìš¸ê¸°ê°€ **ê°ì†Œ**í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµëœë‹¤.

## GAN íŠ¹ì§•
* Not cherry-picked
  * ì´ë¯¸ì§€ë¥¼ ì„ ë³„í•´ì„œ ë„£ì€ ê²Œ ì•„ë‹ˆë¼ ëœë¤í•˜ê²Œ ë„£ì–´ì¤Œ
* **Not memorized** the training set
  * í•™ìŠµ ë°ì´í„°ë¥¼ ë‹¨ìˆœíˆ ì•”ê¸°í•œ ê²ƒì´ ì•„ë‹˜
* Competitive with the better generative models
  * ì´ì „ì˜ ë‹¤ë¥¸ ìƒì„± ëª¨ë¸ê³¼ ë¹„êµí–ˆì„ ë•Œ ì¶©ë¶„íˆ ì¢‹ì€ ì„±ëŠ¥ì´ ë‚˜ì˜´
* Images represent sharp
  * Blurryí•˜ê²Œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì§€ ì•Šê³  ê½¤ ì„ ëª…í•˜ê²Œ ìƒì„±
* ë¬¸ì¥ì´ ê¸¸ì–´ì§ˆìˆ˜ë¡ ìƒì„±ëœ ë¬¸ì¥ì˜ í’ˆì§ˆì´ ì•ˆ ì¢‹ì•„ì§


---
## ğŸ’» ì½”ë“œ ì‹¤ìŠµ - GAN

ì¶œì²˜ : <https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/GAN_for_MNIST_Tutorial.ipynb>

í•´ë‹¹ ì½”ë“œëŠ” MNIST ë°ì´í„°ì…‹ì„ ê°€ì§€ê³  ê°€ì¥ ê¸°ë³¸ì ì¸ GAN ëª¨ë¸ì„ í•™ìŠµí•œ ì½”ë“œì´ë‹¤.

#### ìƒì„±ì Generator ë° íŒë³„ì Discriminator ëª¨ë¸ ì •ì˜
```python
latent_dim = 100

# ìƒì„±ì(Generator) í´ë˜ìŠ¤ ì •ì˜
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()

        # í•˜ë‚˜ì˜ ë¸”ë¡(block) ì •ì˜
        def block(input_dim, output_dim, normalize=True):
            layers = [nn.Linear(input_dim, output_dim)]
            if normalize:
                # ë°°ì¹˜ ì •ê·œí™”(batch normalization) ìˆ˜í–‰(ì°¨ì› ë™ì¼)
                layers.append(nn.BatchNorm1d(output_dim, 0.8))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers

        # ìƒì„±ì ëª¨ë¸ì€ ì—°ì†ì ì¸ ì—¬ëŸ¬ ê°œì˜ ë¸”ë¡ì„ ê°€ì§
        self.model = nn.Sequential(
            *block(latent_dim, 128, normalize=False),
            *block(128, 256),
            *block(256, 512),
            *block(512, 1024),
            nn.Linear(1024, 1 * 28 * 28),
            nn.Tanh()
        )

    def forward(self, z):
        img = self.model(z)
        img = img.view(img.size(0), 1, 28, 28)
        return img
```
```python
# íŒë³„ì(Discriminator) í´ë˜ìŠ¤ ì •ì˜
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()

        self.model = nn.Sequential(
            nn.Linear(1 * 28 * 28, 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 1),
            nn.Sigmoid(),
        )

    # ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„ ê²°ê³¼ë¥¼ ë°˜í™˜
    def forward(self, img):
        flattened = img.view(img.size(0), -1)
        output = self.model(flattened)

        return output
```

#### í•™ìŠµ ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
```python
transforms_train = transforms.Compose([
    transforms.Resize(28),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

train_dataset = datasets.MNIST(root="./dataset", train=True, download=True, transform=transforms_train)
dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)
```

#### ëª¨ë¸ í•™ìŠµ ë° ìƒ˜í”Œë§
```python
# ìƒì„±ì(generator)ì™€ íŒë³„ì(discriminator) ì´ˆê¸°í™”
generator = Generator()
discriminator = Discriminator()

generator.cuda()
discriminator.cuda()

# ì†ì‹¤ í•¨ìˆ˜(loss function)
adversarial_loss = nn.BCELoss()
adversarial_loss.cuda()

# í•™ìŠµë¥ (learning rate) ì„¤ì •
lr = 0.0002

# ìƒì„±ìì™€ íŒë³„ìë¥¼ ìœ„í•œ ìµœì í™” í•¨ìˆ˜
optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))
optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))
```
```python
import time

n_epochs = 200 # í•™ìŠµì˜ íšŸìˆ˜(epoch) ì„¤ì •
sample_interval = 2000 # ëª‡ ë²ˆì˜ ë°°ì¹˜(batch)ë§ˆë‹¤ ê²°ê³¼ë¥¼ ì¶œë ¥í•  ê²ƒì¸ì§€ ì„¤ì •
start_time = time.time()

for epoch in range(n_epochs):
    for i, (imgs, _) in enumerate(dataloader):

        # ì§„ì§œ(real) ì´ë¯¸ì§€ì™€ ê°€ì§œ(fake) ì´ë¯¸ì§€ì— ëŒ€í•œ ì •ë‹µ ë ˆì´ë¸” ìƒì„±
        real = torch.cuda.FloatTensor(imgs.size(0), 1).fill_(1.0) # ì§„ì§œ(real): 1
        fake = torch.cuda.FloatTensor(imgs.size(0), 1).fill_(0.0) # ê°€ì§œ(fake): 0

        real_imgs = imgs.cuda()

        """ ìƒì„±ì(generator)ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. """
        optimizer_G.zero_grad()

        # ëœë¤ ë…¸ì´ì¦ˆ(noise) ìƒ˜í”Œë§
        z = torch.normal(mean=0, std=1, size=(imgs.shape[0], latent_dim)).cuda()

        # ì´ë¯¸ì§€ ìƒì„±
        generated_imgs = generator(z)

        # ìƒì„±ì(generator)ì˜ ì†ì‹¤(loss) ê°’ ê³„ì‚°
        g_loss = adversarial_loss(discriminator(generated_imgs), real)

        # ìƒì„±ì(generator) ì—…ë°ì´íŠ¸
        g_loss.backward()
        optimizer_G.step()

        """ íŒë³„ì(discriminator)ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. """
        optimizer_D.zero_grad()

        # íŒë³„ì(discriminator)ì˜ ì†ì‹¤(loss) ê°’ ê³„ì‚°
        real_loss = adversarial_loss(discriminator(real_imgs), real)
        fake_loss = adversarial_loss(discriminator(generated_imgs.detach()), fake)
        d_loss = (real_loss + fake_loss) / 2

        # íŒë³„ì(discriminator) ì—…ë°ì´íŠ¸
        d_loss.backward()
        optimizer_D.step()

        done = epoch * len(dataloader) + i
        if done % sample_interval == 0:
            # ìƒì„±ëœ ì´ë¯¸ì§€ ì¤‘ì—ì„œ 25ê°œë§Œ ì„ íƒí•˜ì—¬ 5 X 5 ê²©ì ì´ë¯¸ì§€ì— ì¶œë ¥
            save_image(generated_imgs.data[:25], f"{done}.png", nrow=5, normalize=True)

    # epoch 10ë§ˆë‹¤ ë¡œê·¸(log) ì¶œë ¥
    if epoch % 10 == 0:
        print(f"[Epoch {epoch}/{n_epochs}] [D loss: {d_loss.item():.6f}] [G loss: {g_loss.item():.6f}] [Elapsed time: {time.time() - start_time:.2f}s]")
```

#### ìƒì„±ëœ ì´ë¯¸ì§€ ì¶œë ¥
```python
from IPython.display import Image

Image('92000.png')
```

## ì°¸ê³ 

[[YouTube] GAN:Generative Adversarial Networks](https://www.youtube.com/watch?v=AVvlDmhHgC4)

[Make GANs Training Easier for Everyone : Generate Images Following a Sketch](https://www.louisbouchard.ai/make-gans-training-easier/)

ë…¼ë¬¸ Generative Adversarial Networks (NIPS 2014)
---
layout: post
title:  "TIL | ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ì›¹ìŠ¤í¬ë˜í•‘"
tags: [TIL]
date:   2022-09-28 09:05:09 +0900
categories: Python_DataAnalysis
---
# [ 0928 ] ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ì›¹ìŠ¤í¬ë˜í•‘



#### ğŸ‘©ğŸ»â€ğŸ’» ì˜¤ëŠ˜ì½”ë“œ ì‹¤ì‹œê°„ ê°•ì˜ _ ë°•ì¡°ì€ë‹˜
ì˜¤ëŠ˜ë„ ì–´ê¹€ì—†ì´ ì¡°ì€ë‹˜ ëª©ì†Œë¦¬ëŠ” ìŠ¤ìœ—í•˜ì‹œë‹¤ ğŸ¤


## ğŸ“š ì˜¤ëŠ˜ì˜ TIL - ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ì›¹ìŠ¤í¬ë˜í•‘

- [\[ 0928 \] ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ì›¹ìŠ¤í¬ë˜í•‘](#-0928--ë„¤ì´ë²„-ê¸ˆìœµ-ë‰´ìŠ¤-ì›¹ìŠ¤í¬ë˜í•‘)
      - [ğŸ‘©ğŸ»â€ğŸ’» ì˜¤ëŠ˜ì½”ë“œ ì‹¤ì‹œê°„ ê°•ì˜ \_ ë°•ì¡°ì€ë‹˜](#-ì˜¤ëŠ˜ì½”ë“œ-ì‹¤ì‹œê°„-ê°•ì˜-_-ë°•ì¡°ì€ë‹˜)
  - [ğŸ“š ì˜¤ëŠ˜ì˜ TIL - ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ì›¹ìŠ¤í¬ë˜í•‘](#-ì˜¤ëŠ˜ì˜-til---ë„¤ì´ë²„-ê¸ˆìœµ-ë‰´ìŠ¤-ì›¹ìŠ¤í¬ë˜í•‘)
    - [ğŸ¾ã€€ë‰´ìŠ¤ í•œ í˜ì´ì§€ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜ã€€ğŸ¾](#ë‰´ìŠ¤-í•œ-í˜ì´ì§€ë¥¼-ìˆ˜ì§‘í•˜ëŠ”-í•¨ìˆ˜)
    - [ğŸ¾ã€€ ë°˜ë³µë¬¸ì„ ì‚¬ìš©í•´ 10í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘ã€€ğŸ¾](#-ë°˜ë³µë¬¸ì„-ì‚¬ìš©í•´-10í˜ì´ì§€ê¹Œì§€-ìˆ˜ì§‘)
      - [tqdmì˜ trange](#tqdmì˜-trange)
      - [ê·¸ëŸ¼ ì´ì œ 10í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘í•˜ëŠ” ì½”ë“œë¥¼ ë³´ì.](#ê·¸ëŸ¼-ì´ì œ-10í˜ì´ì§€ê¹Œì§€-ìˆ˜ì§‘í•˜ëŠ”-ì½”ë“œë¥¼-ë³´ì)
      - [10í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ë³´ì.](#10í˜ì´ì§€ê¹Œì§€-ìˆ˜ì§‘í•˜ëŠ”-í•¨ìˆ˜ë¥¼-ë§Œë“¤ì–´ë³´ì)
    - [ğŸ¾ã€€ë„¤ì´ë²„ ê¸ˆìœµ ê°œë³„ì¢…ëª© ìˆ˜ì§‘ã€€ğŸ¾](#ë„¤ì´ë²„-ê¸ˆìœµ-ê°œë³„ì¢…ëª©-ìˆ˜ì§‘)
      - [1. url ì–»ì–´ì˜¤ê¸°](#1-url-ì–»ì–´ì˜¤ê¸°)
      - [2. Requests ëª¨ë“ˆ ì‚¬ìš©í•´ URLì— ì ‘ê·¼](#2-requests-ëª¨ë“ˆ-ì‚¬ìš©í•´-urlì—-ì ‘ê·¼)
      - [3. response.status\_codeê°€ 200 OKë¼ë©´ ì •ìƒ ì‘ë‹µ](#3-responsestatus_codeê°€-200-okë¼ë©´-ì •ìƒ-ì‘ë‹µ)
      - [4. responseì˜ text ê°’ ë°›ì•„ì˜¤ê¸°](#4-responseì˜-text-ê°’-ë°›ì•„ì˜¤ê¸°)
      - [5. html textë¥¼ BeautifulSoupì„ í†µí•´ html í•´ì„](#5-html-textë¥¼-beautifulsoupì„-í†µí•´-html-í•´ì„)
        - [ğŸ’¡ find\_allê³¼ selectì˜ ì°¨ì´ëŠ” ë­˜ê¹Œ?](#-find_allê³¼-selectì˜-ì°¨ì´ëŠ”-ë­˜ê¹Œ)
      - [6. soup.selectë¥¼ í†µí•´ ì›í•˜ëŠ” íƒœê·¸ì— ì ‘ê·¼](#6-soupselectë¥¼-í†µí•´-ì›í•˜ëŠ”-íƒœê·¸ì—-ì ‘ê·¼)
    - [ğŸ¾ã€€ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ã€€ğŸ¾](#ë°ì´í„°-ìˆ˜ì§‘-ì‹œì‘)
      - [í˜ì´ì§€ë³„ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ ë§Œë“¤ê¸°](#í˜ì´ì§€ë³„-ë°ì´í„°-ìˆ˜ì§‘-í•¨ìˆ˜-ë§Œë“¤ê¸°)
      - [ë°˜ë³µë¬¸ì„ í†µí•œ ì „ì²´ ì¼ì ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°](#ë°˜ë³µë¬¸ì„-í†µí•œ-ì „ì²´-ì¼ì-ë°ì´í„°-ìˆ˜ì§‘í•˜ê¸°)
      - [ë°ì´í„°í”„ë ˆì„ì— ì¢…ëª©ì½”ë“œì™€ ì¢…ëª©ëª…ì„ ì¶”ê°€í•˜ê¸°](#ë°ì´í„°í”„ë ˆì„ì—-ì¢…ëª©ì½”ë“œì™€-ì¢…ëª©ëª…ì„-ì¶”ê°€í•˜ê¸°)
      - [ìœ„ì˜ í•˜ë‚˜ë¡œ í•©ì³ì„œ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ ë§Œë“¤ê¸°](#ìœ„ì˜-í•˜ë‚˜ë¡œ-í•©ì³ì„œ-íŒŒì¼ë¡œ-ì €ì¥í•˜ëŠ”-í•¨ìˆ˜-ë§Œë“¤ê¸°)
    - [ë‹¤ìŒ í¬ìŠ¤íŠ¸ì—ì„œ ë§Œë‚˜ìš” ğŸ™Œ](#ë‹¤ìŒ-í¬ìŠ¤íŠ¸ì—ì„œ-ë§Œë‚˜ìš”-)




### ğŸ¾ã€€ë‰´ìŠ¤ í•œ í˜ì´ì§€ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜ã€€ğŸ¾
ë‹¤ìŒì€ ì €ë²ˆ ì‹œê°„ ê³¼ì œë¡œ ë‚´ê°€ ì™„ì„±í•œ ì½”ë“œì´ë‹¤. ë‰´ìŠ¤ í•œ í˜ì´ì§€ë¥¼ ë°›ì•„ì™€ì„œ í˜ì´ì§€ ì •ë³´ëŠ” ì œì™¸í•˜ê³  ì»¬ëŸ¼ëª…ì„ í†µì¼ì‹œì¼œì„œ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë§Œë“¤ì—ˆë‹¤. 
```python
# get_one_page_news í•¨ìˆ˜ ë§Œë“¤ê¸°
def get_one_page_news(item_code, page_no):
    """
    get_url ì— item_code, page_no ë¥¼ ë„˜ê²¨ url ì„ ë°›ì•„ì˜¤ê³ 
    ë‰´ìŠ¤ í•œ í˜ì´ì§€ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
    1) URL ì„ ë°›ì•„ì˜´
    2) read_html ë¡œ í…Œì´ë¸” ì •ë³´ë¥¼ ë°›ì•„ì˜´
    3) ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ëª…ì„ ["ì œëª©", "ì •ë³´ì œê³µ", "ë‚ ì§œ"]ë¡œ ë³€ê²½
    4) temp_list ì— ë°ì´í„°í”„ë ˆì„ì„ ì¶”ê°€
    5) concat ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ ë³‘í•©í•˜ì—¬ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë§Œë“¤ê¸°
    6) ê²°ì¸¡ì¹˜ ì œê±°
    7) ì—°ê´€ê¸°ì‚¬ ì œê±°
    8) ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
    """
    # get_urlë¡œ url ë°›ì•„ì˜¤ê¸°
    url = get_url(item_code, page_no)   

    # read_htmlë¡œ í…Œì´ë¸” ì •ë³´ ë°›ì•„ì˜¤ê¸°
    table = pd.read_html(url)[:-1]
    
    # ë°ì´í„° ì»¬ëŸ¼ëª… ë³€ê²½
    cols = ["ì œëª©", "ì •ë³´ì œê³µ", "ë‚ ì§œ"]
    temp_list = []
    for news in table:
        news.columns = cols
        temp_list.append(news)
        # display(news)
    
    # concatìœ¼ë¡œ í•˜ë‚˜ì˜ DataFrame
    df_news = pd.concat(temp_list)

    # ê²°ì¸¡ì¹˜ ì œê±° / ì¤‘ë³µ ì œê±°
    df_news = df_news.dropna()
    df_news_page = df_news.drop_duplicates()
    
    # ì—°ê´€ê¸°ì‚¬ ì œê±°
    df_news_page = df_news_page[~df_news_page["ì •ë³´ì œê³µ"].str.contains("ì—°ê´€ê¸°ì‚¬")].copy()

    # ì¸ë±ìŠ¤ ì¬ì„¤ì •
    df_news_page = df_news_page.reset_index(drop = True)

    return df_news_page
```
í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì˜ ë¶ˆëŸ¬ì™€ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.
```python
get_one_page_news(item_code, page_no).head()
```


### ğŸ¾ã€€ ë°˜ë³µë¬¸ì„ ì‚¬ìš©í•´ 10í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘ã€€ğŸ¾
#### tqdmì˜ trange
tqdmì˜ trangeë¥¼ ì´ìš©í•˜ì—¬ 10í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘í•  ì˜ˆì •ì´ë‹¤. ë¨¼ì € tqdmì˜ trangeì´ ë­”ì§€ì— ëŒ€í•´ ì•Œì•„ë³´ê² ë‹¤.
```python
import time
from tqdm import trange

page_no = 0
news_list = []
for i in trange(100):
    i == 1

```
![trange](/assets/img/img_220928/trange.png){: width="70%" height="70%"} <br/>
ìœ„ì²˜ëŸ¼ ì§„í–‰ë°”ê°€ ë³´ì´ë©´ì„œ ë°˜ë³µë¬¸ì„ ëŒê²Œ ëœë‹¤.

#### ê·¸ëŸ¼ ì´ì œ 10í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘í•˜ëŠ” ì½”ë“œë¥¼ ë³´ì.
```python
import time
from tqdm import trange

# ì¹´ì¹´ì˜¤ì˜ ì •ë³´ ë°›ì•„ì˜¤ê¸°
item_code = df_krx.loc[df_krx["Name"] == "ì¹´ì¹´ì˜¤", "Symbol"].values[0]

news_list = []
for page_no in trange(1,11):
    temp = get_one_page_news(item_code, page_no)
    news_list.append(temp)
    # ì„œë²„ì— ë¬´ë¦¬ê°€ ê°€ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ ë”œë ˆì´ë¥¼ ê±¸ì–´ì£¼ì. 
    time.sleep(0.1) 
```

#### 10í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ë³´ì.


<br/><br/><br/>

### ğŸ¾ã€€ë„¤ì´ë²„ ê¸ˆìœµ ê°œë³„ì¢…ëª© ìˆ˜ì§‘ã€€ğŸ¾
![samsung_finance](/assets/img/img_220928/samsung_finance.png){: width="100%" height="100%"} <br/>
#### 1. url ì–»ì–´ì˜¤ê¸°
**ê²€ì‚¬ > ë„¤íŠ¸ì›Œí¬ > Docs > Request**ì— ìˆëŠ” urlì„ get <br/>
![url_get](/assets/img/img_220928/url_get.png){: width="70%" height="70%"} <br/><br/>
ì—¬ê¸°ì„œ ì–»ì€ urlë¡œ ë“¤ì–´ê°€ê²Œ ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ 'ì¼ë³„ì‹œì„¸'ì—ë§Œ í•´ë‹¹í•˜ëŠ” í˜ì´ì§€ê°€ ì—´ë¦°ë‹¤.<br/>
![daily_sise](/assets/img/img_220928/daily_sise.png) <br/><br/>
`<table>` tag ì•ˆì— ìˆì–´ë„ `pd.read_html(url)`ë¡œ ë°ì´í„°ë¥¼ ëª» ê°€ì ¸ì˜¤ëŠ” ê²½ìš°ë„ ìˆë‹¤. ì´ë•Œ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ `ValueError: No tables found`ë¼ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí•œë‹¤. <br/>
![valueerror_notables](/assets/img/img_220928/valueerror_notables.png) <br/><br/>
ì´ëŠ” ì •ìƒì ìœ¼ë¡œ ì •ë³´ë¥¼ ìš”ì²­í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì´ë•Œ Selenium ì´ìš©í•˜ë©´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ìˆì§€ë§Œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤. -> **Requests**ë¥¼ ì‚¬ìš©í•˜ì.

#### 2. Requests ëª¨ë“ˆ ì‚¬ìš©í•´ URLì— ì ‘ê·¼
- **Requests** ê³µì‹ ë¬¸ì„œ : 
[**[Requests : HTTP for Humans]**](https://requests.readthedocs.io/en/latest/)
```python
response = requests.get(url)
response
# <Response [200]>
```

#### 3. response.status_codeê°€ 200 OKë¼ë©´ ì •ìƒ ì‘ë‹µ
[HTTP ìƒíƒœ ì½”ë“œì— ëŒ€í•´ ì•Œì•„ë³´ê¸°](https://ko.wikipedia.org/wiki/HTTP_%EC%83%81%ED%83%9C_%EC%BD%94%EB%93%9C)ë¥¼ ì°¸ê³ í•˜ë©´ ì´ë•Œ ì¶œë ¥ë˜ëŠ” ìƒíƒœì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œì•„ë³¼ ìˆ˜ ìˆë‹¤. <br/>
`Response [200]`ê°€ ë‚˜ì˜¤ë©´ ìš”ì²­ì„ ì„±ê³µì ìœ¼ë¡œ ë°›ì•˜ìœ¼ë©° ì¸ì‹í–ˆê³  ìˆ˜ìš©í–ˆë‹¤ëŠ” ëœ»ì´ë‹¤.

#### 4. responseì˜ text ê°’ ë°›ì•„ì˜¤ê¸°
```python
reponse.text
```
ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ 'ë°©ë¬¸í•˜ì‹œë ¤ëŠ” í˜ì´ì§€ì˜ ì£¼ì†Œê°€ ì˜ëª» ì…ë ¥ë˜ì—ˆê±°ë‚˜, ...'ì™€ ê°™ì€ ì˜¤ë¥˜ë©”ì‹œì§€ê°€ ì¶œë ¥ëœë‹¤. ì´ ë˜í•œ ì •ìƒì ì¸ ìš”ì²­ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì´ë‹¤. <br/>
ì •ìƒì ìœ¼ë¡œ ìš”ì²­ì„ ë³´ë‚´ë ¤ë©´ headerë¥¼ ë¨¼ì € ë´ì•¼í•œë‹¤. header ì¤‘ **user agent**ë¥¼ ê°€ì ¸ì™€ ì½”ë“œì— ì¶”ê°€í•´ì£¼ë©´ ì´ ì´ìŠˆëŠ” í•´ê²°ëœë‹¤! ë¡œë´‡ì´ ì•„ë‹ˆë¼ ë¸Œë¼ìš°ì €ë¥¼ ì“°ëŠ” ì‚¬ìš©ìë¼ëŠ” ëœ»ì„ ì˜ë¯¸í•œë‹¤. ë‚´ ê²½ìš°ì—” user-agentê°€ 'Mozilla/5.0'ì˜€ë‹¤.
```python
print(url)
response = requests.get(url, headers={"user-agent": "Mozilla/5.0"})
response
```
```python
response.text
```
ìœ„ì˜ ì½”ë“œë¥¼ ì¬ì‹¤í–‰í•´ì£¼ë©´ ë‹¤ìŒê³¼ ê°™ì€ 'ì¼ë³„ì‹œì„¸'ì— í•´ë‹¹í•˜ëŠ” `text`ê°’ì´ ì¶œë ¥ëœë‹¤.<br/>
![response_text_front](/assets/img/img_220928/response_text_front.png) <br/><br/>

#### 5. html textë¥¼ BeautifulSoupì„ í†µí•´ html í•´ì„

requestsë§Œì„ ê°€ì§€ê³  ë°ì´í„°ë¥¼ ì˜ ê°€ì ¸ì™”ì§€ë§Œ êµ¬ì¡°ë¥¼ í•œ ëˆˆì— ë³´ê¸° ì–´ë µë‹¤. ì´ë•Œ ì´ìš©í•˜ëŠ” ê²ƒì´ BeautifulSoupì´ë‹¤. ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰í•´ ê²°ê³¼ë¥¼ ë³´ê² ë‹¤. 
```python
from bs4 import BeautifulSoup as bs
html = bs(response.text)
```
![bs_text](/assets/img/img_220928/bs_text.png){: width="70%" height="70%"} <br/><br/>
ì´ì²˜ëŸ¼ êµ¬ì¡°ì ìœ¼ë¡œ textë¥¼ ì¶œë ¥ì´ ëœë‹¤. ëˆˆì´ í¸ì•ˆí•´ì§€ëŠ” ëŠë‚Œ. <br/>
ì´ì œ [BeautifulSoup ê³µì‹ ë¬¸ì„œ](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)ë¥¼ í™•ì¸í•´ì„œ ëª‡ ê°€ì§€ ë°ì´í„°ë¥¼ ë” í™•ì¸í•´ë³´ì.
```python
html.title
# <title>ë„¤ì´ë²„ ê¸ˆìœµ</title>

html.a
# <a href="/item/sise_day.naver?code=005930&amp;page=1">1</a>

html.find_all("a")[:3]
# [<a href="/item/sise_day.naver?code=005930&amp;page=1">1</a>,
#  <a href="/item/sise_day.naver?code=005930&amp;page=2">2</a>,
#  <a href="/item/sise_day.naver?code=005930&amp;page=3">3</a>]
```

```python
html.select("a")
# [<a href="/item/sise_day.naver?code=005930&amp;page=1">1</a>,
#  <a href="/item/sise_day.naver?code=005930&amp;page=2">2</a>,
#  <a href="/item/sise_day.naver?code=005930&amp;page=3">3</a>,
#  <a href="/item/sise_day.naver?code=005930&amp;page=4">4</a>,
#  <a href="/item/sise_day.naver?code=005930&amp;page=5">5</a>,
#  <a href="/item/sise_day.naver?code=005930&amp;page=6">6</a>,
#  <a href="/item/sise_day.naver?code=005930&amp;page=7">7</a>,
#  <a href="/item/sise_day.naver?code=005930&amp;page=8">8</a>,
#  <a href="/item/sise_day.naver?code=005930&amp;page=9">9</a>,
#  <a href="/item/sise_day.naver?code=005930&amp;page=10">10</a>,
#  <a href="/item/sise_day.naver?code=005930&amp;page=11">
#  				ë‹¤ìŒ<img alt="" border="0" height="5" src="https://ssl.pstatic.net/static/n/cmn/bu_pgarR.gif" width="3"/>
#  </a>,
#  <a href="/item/sise_day.naver?code=005930&amp;page=660">ë§¨ë’¤
#  				<img alt="" border="0" height="5" src="https://ssl.pstatic.net/static/n/cmn/bu_pgarRR.gif" width="8"/>
#  </a>]
html.table
```
##### ğŸ’¡ find_allê³¼ selectì˜ ì°¨ì´ëŠ” ë­˜ê¹Œ?

: find_allëŠ” ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ê°€ ìˆê³  attributeë¥¼ ì´ìš©í•´ **ì½• ì§‘ì–´ì„œ** íŠ¹ì • ë§í¬ì—ë§Œ ìˆëŠ” ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê²Œ í•  ìˆ˜ ìˆë‹¤. selectëŠ” inspectì— ë“¤ì–´ê°€ì„œ selectorë¥¼ ë³µì‚¬í•´ì˜¬ ìˆ˜ ìˆì–´ í¸í•˜ê²Œ í™œìš©í•  ìˆ˜ ìˆìœ¼ë©°, hierachicalí•˜ê²Œ ì¶œë ¥í•´ì¤€ë‹¤. <br/>

![findall_example](/assets/img/img_220928/findall_example.png){: width="70%" height="70%"} <br/>
```python
### ê·œí˜¸ë‹˜ì˜ ì¹œì ˆí•œ ì„¤ëª… ###
#find
soup.find('div').find('p')
#select
soup.select_one('div > p')
``` 

<br/>

#### 6. soup.selectë¥¼ í†µí•´ ì›í•˜ëŠ” íƒœê·¸ì— ì ‘ê·¼
> **select** ê¸°ëŠ¥ì€ ì•Œê³  ìˆìœ¼ë©´ ì¢‹ë‹¤. - ì¡°ì€ë‹˜

selectëŠ” ì´ëŸ° ì‹ìœ¼ë¡œ ì‘ìš©í•  ìˆ˜ ìˆë‹¤. ì¤‘ê°„ì— tagë¥¼ ê±´ë„ˆë›°ë©´ '[]'ì™€ ê°™ì´ ì•„ë¬´ê²ƒë„ ì¶œë ¥ë˜ì§€ ì•Šìœ¼ë‹ˆ êµ¬ì¡°ë¥¼ ì˜ ë³´ê³  ì ìš©í•˜ë„ë¡ í•˜ì. 

```python
# body > table.Nnavi
html.select("table.Nnavi")
############## ì¶œë ¥ê²°ê³¼ ##############
# [<caption>í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜</caption>]
####################################

# body > table.type2 > tr > th
html.select("body > table.type2 > tr > th")
############## ì¶œë ¥ê²°ê³¼ ##############
# [<th>ë‚ ì§œ</th>,
#  <th>ì¢…ê°€</th>,
#  <th>ì „ì¼ë¹„</th>,
#  <th>ì‹œê°€</th>,
#  <th>ê³ ê°€</th>,
#  <th>ì €ê°€</th>,
#  <th>ê±°ë˜ëŸ‰</th>]
####################################
```

### ğŸ¾ã€€ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ã€€ğŸ¾

#### í˜ì´ì§€ë³„ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ ë§Œë“¤ê¸°
ë‹¤ìŒì€ íŠ¹ì • ì¢…ëª©ì˜ íŠ¹ì • í˜ì´ì§€ì— ìˆëŠ” ë°ì´í„°(table)ì„ ìˆ˜ì§‘í•´ì˜¤ëŠ” í•¨ìˆ˜ ì½”ë“œì´ë‹¤. 
```python
def get_day_list(item_code, page_no):
    """
    ì¼ìë³„ ì‹œì„¸ë¥¼ í˜ì´ì§€ë³„ë¡œ ìˆ˜ì§‘
    == sudo code ==
    1. urlì„ ë§Œë“ ë‹¤.
    2. requestsë¥¼ í†µí•´ html ë¬¸ì„œë¥¼ ë°›ì•„ì˜¨ë‹¤.
    3. read_htmlì„ í†µí•´ table íƒœê·¸ë¥¼ ì½ì–´ì˜¨ë‹¤.
    4. ê²°ì¸¡í–‰ì„ ì œê±°
    5. ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
    """ 
    # 1. urlì„ ë§Œë“ ë‹¤.
    url = f"https://finance.naver.com/item/sise_day.naver?code={item_code}&page={page_no}"

    # 2. requestsë¥¼ í†µí•´ html ë¬¸ì„œë¥¼ ë°›ì•„ì˜¨ë‹¤.
    response = requests.get(url, headers = {"user-agent": "Mozilla/5.0"})

    # 3. read_htmlì„ í†µí•´ table íƒœê·¸ë¥¼ ì½ì–´ì˜¨ë‹¤.
    data = pd.read_html(response.text)[0]

    # 4. ê²°ì¸¡í–‰ì„ ì œê±°
    data = data.dropna()

    # 5. ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
    return data
```
ì´ì œ ì½”ë“œê°€ ì˜ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€ í™•ì¸í•´ë³´ì.
```python
item_code = "005930"
item_name = "ì‚¼ì„±ì „ì"
page_no = 1
```
```python
# í•¨ìˆ˜ê°€ ì˜ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€ í™•ì¸í•˜ê¸°
get_day_list(item_code, page_no)
```
ë‹¤ìŒê³¼ ê°™ì´ ë°ì´í„°ë¥¼ ì˜ ë°›ì•„ì˜¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. `item_code`ì™€ `page_no`ë¥¼ ë°”ê¿”ì¤˜ë„ ì˜ ì‘ë™í•œë‹¤. <br/>
![get_day_list_ex](/assets/img/img_220928/get_day_list_ex.png){: width="70%" height="70%"} <br/>
<br/><br/>


#### ë°˜ë³µë¬¸ì„ í†µí•œ ì „ì²´ ì¼ì ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°
> ê¸°ê°„ì´ ê¸´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ë•ŒëŠ” ì„œë²„ì— ë¶€ë‹´ì„ ì£¼ì§€ ì•Šê¸° ìœ„í•´ time.sleep()ì„ í•´ì¤€ë‹¤. - ì¡°ì€ë‹˜

ê·¸ ì¤‘ì— ìš°ë¦¬ëŠ” ìµœê·¼ì— ìƒì¥í•œ ì¢…ëª© ì¤‘ì— **ì˜ì¹´**ì˜ ì •ë³´ë¥¼ ë°›ì•„ì˜¤ë„ë¡ í•˜ê² ë‹¤. ë°ì´í„°ê°€ ì ìœ¼ë‹ˆê¹Œ ã….ã…
```python
# ìµœê·¼ ìƒì¥ ì¢…ëª© ì¤‘ ì¶”ì²œ ì£¼ -> ì˜ì¹´
item_code = "403550"
item_name = "ì˜ì¹´"
```
í˜„ì¬ ì˜ì¹´ì˜ ì¼ë³„ì‹œì„¸ëŠ” 3pageê¹Œì§€ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ page_noê°€ 4ë¡œ ì…ë ¥ì´ ë˜ì–´ë„ 3page ì •ë³´ê°€ ë‚˜ì˜¤ê³ , 10ìœ¼ë¡œ ì…ë ¥ì´ ë˜ì–´ë„ 3page ì •ë³´ê°€ ì¤‘ë³µí•´ì„œ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— ì´ë¥¼ ë°˜ë³µë¬¸ìœ¼ë¡œ ì œì–´í•˜ë„ë¡ í•˜ì.  

```python
import time

# web page ì‹œì‘ë²ˆí˜¸
page_no = 1

# ë°ì´í„°ë¥¼ ì €ì¥í•  ë¹ˆ ë³€ìˆ˜ ì„ ì–¸
item_list = []
curr_day = ""
while True:
    # í˜ì´ì§€ë³„ ë°ì´í„° ìˆ˜ì§‘í•´ì˜¤ê¸°
    df_item = get_day_list(item_code, page_no)

    # ë§ˆì§€ë§‰ ë‚ ì§œë¥¼ ê°€ì ¸ì˜¤ì
    last_day = df_item.iloc[-1]["ë‚ ì§œ"]
    print(page_no, curr_day, last_day)

    # ê°™ì€ í˜ì´ì§€ë¼ë©´ ë°˜ë³µë¬¸ íƒˆì¶œ
    if last_day == curr_day:
        break

    item_list.append(df_item)
    page_no += 1

    # curr_day ê°±ì‹ í•´ì£¼ê¸°
    curr_day = last_day

    time.sleep(0.1)
```
ì´ë ‡ê²Œ í˜ì´ì§€ë³„ë¡œ ì–»ì€ item_listë¥¼ concat ì‹œì¼œì£¼ì.
```python
df_day = pd.concat(item_list).reset_index(drop = True)
```
![soca_concat](/assets/img/img_220928/soca_concat.png){: width="70%" height="70%"} <br/>

#### ë°ì´í„°í”„ë ˆì„ì— ì¢…ëª©ì½”ë“œì™€ ì¢…ëª©ëª…ì„ ì¶”ê°€í•˜ê¸°
```python
df_day["ì¢…ëª©ì½”ë“œ"] = item_code
df_day["ì¢…ëª©ëª…"] = item_name
df_day.head(5)
```
![df_day](/assets/img/img_220928/df_day.png){: width="70%" height="70%"} <br/><br/>

ì»¬ëŸ¼ ìˆœì„œë¥¼ ë³€ê²½í•´ì£¼ì. ë‹¤ìŒ col ë¦¬ìŠ¤íŠ¸ ìˆœì„œëŒ€ë¡œ ë³€ê²½í•˜ëŠ” ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
```python
cols = ['ì¢…ëª©ì½”ë“œ', 'ì¢…ëª©ëª…', 'ë‚ ì§œ', 'ì¢…ê°€', 'ì „ì¼ë¹„', 'ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ê±°ë˜ëŸ‰']
df_day = df_day[cols]
df_day.head(5)
```
![df_day_recol](/assets/img/img_220928/df_day_recol.png){: width="70%" height="70%"} <br/><br/>

ê·¸ëŸ¼ ì´ì œ ì¤‘ë³µë°ì´í„°ë¥¼ ì œê±°í•´ë³´ì.
```python
# drop_duplicates : rowë“¤ ë¼ë¦¬ dataë¥¼ ë¹„êµí•˜ì—¬ ê°™ì€ ê°’ì´ ìˆìœ¼ë©´ rowì¤‘ í•˜ë‚˜ë¥¼ ì‚­ì œ
# df.shapeë¥¼ ì „/í›„ë¡œ ì¶œë ¥í•˜ì—¬ ì‚­ì œëœ rowê°€ ìˆëŠ”ì§€ í™•ì¸
df_day = df_day.drop_duplicates()
```

#### ìœ„ì˜ í•˜ë‚˜ë¡œ í•©ì³ì„œ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ ë§Œë“¤ê¸°
ìœ„ì—ì„œ ë°°ìš´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ëª¨ë“  ê³¼ì •ì„ í•œë²ˆì— ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ë³´ì•˜ë‹¤. 
```python
def get_item_list(item_code, item_name):
    """
    ì¼ë³„ ì‹œì„¸ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
    
    """
    # web page ì‹œì‘ë²ˆí˜¸
    page_no = 1
    # ë°ì´í„°ë¥¼ ì €ì¥í•  ë¹ˆ ë³€ìˆ˜ ì„ ì–¸
    item_list = []
    curr_day = ""
    while True:
        # í˜ì´ì§€ë³„ ë°ì´í„° ìˆ˜ì§‘í•´ì˜¤ê¸°
        df_item = get_day_list(item_code, page_no)

        # ë§ˆì§€ë§‰ ë‚ ì§œë¥¼ ê°€ì ¸ì˜¤ì
        last_day = df_item.iloc[-1]["ë‚ ì§œ"]
        # print(page_no, curr_day, last_day)

        # ê°™ì€ í˜ì´ì§€ë¼ë©´ ë°˜ë³µë¬¸ íƒˆì¶œ
        if last_day == curr_day:
            break

        item_list.append(df_item)
        page_no += 1

        # curr_day ê°±ì‹ í•´ì£¼ê¸°
        curr_day = last_day
        time.sleep(0.1)
    
    # í˜ì´ì œë³„ë¡œ ì–»ì€ ë°ì´í„°, ì¦‰ item_listë¥¼ í•˜ë‚˜ë¡œ í•©ì³ì£¼ê¸°
    df_day = pd.concat(item_list).reset_index(drop = True)
    df_day["ì¢…ëª©ì½”ë“œ"] = item_code
    df_day["ì¢…ëª©ëª…"] = item_name
    cols = ['ì¢…ëª©ì½”ë“œ', 'ì¢…ëª©ëª…', 'ë‚ ì§œ', 'ì¢…ê°€', 'ì „ì¼ë¹„', 'ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ê±°ë˜ëŸ‰']
    df_day = df_day[cols].drop_duplicates()

    # ìµœê·¼ ë‚ ì§œ êµ¬í•´ì„œ 'ì¢…ëª©ëª…_ì¢…ëª©ì½”ë“œ_ë‚ ì§œ'ë¡œ íŒŒì¼ì´ë¦„ ì €ì¥
    date = df_day.iloc[0]["ë‚ ì§œ"]
    file_name = f"{item_name}_{item_code}_{date}.csv"
    df_day.to_csv(file_name, index = False)
    
    return df_day
```
í•¨ìˆ˜ê°€ ì˜ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€ ì˜ì¹´ì™€ ì¹´ì¹´ì˜¤ ì¢…ëª©ì„ ëŒ€ì…í•´ë³´ì. 
```python
item_code = "403550"
item_name = "ì˜ì¹´"

get_item_list(item_code, item_name)
```

![get_item_list_soca](/assets/img/img_220928/get_item_list_soca.png) <br/><br/>
```python
item_code = "323410"
item_name = "ì¹´ì¹´ì˜¤ë±…í¬"

get_item_list(item_code, item_name)
```
![get_item_list_kakao](/assets/img/img_220928/get_item_list_kakao.png) <br/><br/>

### ë‹¤ìŒ í¬ìŠ¤íŠ¸ì—ì„œ ë§Œë‚˜ìš” ğŸ™Œ
ë’· ë‚´ìš©ì€ [**[0928] ETF ìˆ˜ì§‘**](https://seul1230.github.io/2022_likelion/2022-09-28-likelion-TIL2/)ì—ì„œ ì´ì–´ì„œ ì‘ì„±í•œë‹¤.
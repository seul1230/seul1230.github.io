---
layout: post
title:  "2022_likelion TIL"
date:   2022-11-23 14:00:09 +0900
categories: Python_DataAnalysis
---
# [ 1123 ] Kaggle - Benz Boosting Model
## ğŸ‘©ğŸ»â€ğŸ’» ì˜¤ëŠ˜ì½”ë“œ ì‹¤ì‹œê°„ ê°•ì˜ _ ë°•ì¡°ì€ë‹˜
ì˜¤ëŠ˜ë„ í™”ì´íŒ… :)

ì˜¤ëŠ˜ë„ **[[ Kaggle ] Mercedes-Benz Greener Manufacturing](https://www.kaggle.com/competitions/mercedes-benz-greener-manufacturing/submissions)** ë°ì´í„°ë¥¼ ê°€ì§€ê³  ì‹¤ìŠµì„ ì§„í–‰í•˜ì˜€ë‹¤. í•´ë‹¹ í¬ìŠ¤íŠ¸ëŠ” ë¶€ìŠ¤íŒ… ëª¨ë¸ 3ëŒ€ì¥ XGBoost, LightGBM, CatBoostë¥¼ ì ìš©í•œ ë‚´ìš©ì— ëŒ€í•´ ì‘ì„±í•  ì˜ˆì •ì´ë‹¤.

![](/assets/img/img_221124/kaggle_benz.png){: .center }

ì¶œì²˜ : [Mercedes-Benz Greener Manufacturing](https://www.kaggle.com/competitions/mercedes-benz-greener-manufacturing/overview)

<!-- ğŸ“™ ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” **ì´ë¡  ë° ê°œë…**ì„ ì¤‘ì‹¬ì ìœ¼ë¡œ ë‹¤ë£° ì˜ˆì •ì´ë‹¤. -->

<br/>

***


## ğŸ“™ ì´ë¡  ë° ê°œë… 

<!-- ### 1. Baggingê³¼ Boosting
#### Bagging
* Bootstrap Agreggatingì˜ ì•½ì
* ë°ì´í„°ì—ì„œ ì¤‘ë³µì„ í—ˆìš©í•˜ì—¬ ì› ë°ì´í„°ì…‹ê³¼ ê°™ì€ í¬ê¸°ì˜ ë°ì´í„°ì…‹ì„ ë§Œë“œëŠ” ê³¼ì •ì„ ë§í•œë‹¤.
* ëª¨ë“  íŠ¸ë¦¬ë“¤ì„ ë™ì¼í•œ ë°ì´í„°ì…‹ìœ¼ë¡œë§Œ í›ˆë ¨ì‹œí‚¤ê²Œ ë˜ë©´, íŠ¸ë¦¬ë“¤ì˜ ìƒê´€ì„±(correlation)ì€ êµ‰ì¥íˆ ì»¤ì§ˆ ê²ƒì´ë‹¤. 
* ë”°ë¼ì„œ ë°°ê¹…ì€ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ì…‹ë“¤ì— ëŒ€í•´ í›ˆë ¨ ì‹œí‚´ìœ¼ë¡œì¨, íŠ¸ë¦¬ë“¤ì„ ë¹„ìƒê´€í™”ì‹œì¼œ ì¤€ë‹¤.
```
1. ë¶€íŠ¸ìŠ¤íŠ¸ë© ë°©ë²•ì„ í†µí•´ Tê°œì˜ í›ˆë ¨ ë°ì´í„°ì…‹ì„ ìƒì„±í•œë‹¤.
2. Tê°œì˜ ê¸°ì´ˆ ë¶„ë¥˜ê¸°(íŠ¸ë¦¬)ë“¤ì„ í›ˆë ¨ì‹œí‚¨ë‹¤.
3. íŠ¸ë¦¬ë“¤ì„ í•˜ë‚˜ì˜ ë¶„ë¥˜ê¸°ë¡œ ê²°í•©í•œë‹¤. (RandomForest)
```

#### Boosting
  * ì—¬ëŸ¬ ì–•ì€ íŠ¸ë¦¬ë¥¼ ì—°ê²°í•˜ì—¬ í¸í–¥ê³¼ ë¶„ì‚°ì„ ì¤„ì—¬ì„œ ê°•ë ¥í•œ íŠ¸ë¦¬ë¥¼ ìƒì„±
  * ì´ì „ íŠ¸ë¦¬ì—ì„œ í‹€ë ¸ë˜ ë¶€ë¶„ì— ê°€ì¤‘ì¹˜ë¥¼ ì£¼ë©° ì§€ì†ì ìœ¼ë¡œ í•™ìŠµí•´ ë‚˜ê°„ë‹¤.

#### ğŸ’¡ Baggingê³¼ Boosting
* **Bagging** : **<font color='red'>overfitting</font>** ë¬¸ì œì— ì í•©
* **Boosting** : **<font color='red'>ê°œë³„ íŠ¸ë¦¬ ì„±ëŠ¥</font>** ì´ ì¤‘ìš”í•  ë•Œ ì£¼ë¡œ ì‚¬ìš©

### 2. ì„ í˜• íšŒê·€ <font color='lightgray'>Linear Regression</font>

1. ë‹¤ë¥¸ ëª¨ë¸ì— ë¹„í•´ ì›ë¦¬ê°€ ê°„ë‹¨í•˜ë‹¤
2. í•™ìŠµ ì†ë„ê°€ ë§¤ìš° ë¹ ë¥´ë‹¤
3. ì¡°ì •í•´ ì¤„ íŒŒë¼ë¯¸í„°ê°€ ì ë‹¤
4. ì´ìƒì¹˜ì— ì˜í–¥ì„ í¬ê²Œ ë°›ëŠ”ë‹¤.
5. ë°ì´í„°ê°€ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¡œë§Œ ì´ë£¨ì–´ì ¸ìˆì„ ê²½ìš°, ë°ì´í„°ì˜ ê²½í–¥ì„±ì´ ëšœë ·í•  ê²½ìš°, ì‚¬ìš©í•˜ê¸°ì— ì¢‹ë‹¤.

#### ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ ë‹¨ì ì„ ë³´ì™„í•œ ëª¨ë¸
* Ridge
* Lasso
* ElasticNet


### 3. Bagging

#### âœ… RandomForest



#### âœ… Extra Tree
![](/assets/img/img_221124/extratree.png){: .center width="80%"}
* ê·¹ë„ë¡œ Randomizedëœ ëª¨ë¸
* Random Forestì™€ ê°™ì´ í›„ë³´ ê¸°ëŠ¥ì˜ ë¬´ì‘ìœ„ í•˜ìœ„ ì§‘í•©ì´ ì‚¬ìš©ë¨
* ê°€ì¥ ì°¨ë³„ì ì¸ ì„ê³„ê°’ì„ ì°¾ëŠ” ëŒ€ì‹  ê° í›„ë³´ ê¸°ëŠ¥ì— ëŒ€í•´ ì„ê³„ê°’ì´ ë¬´ì‘ìœ„ë¡œ ê·¸ë ¤ì§
* ë¬´ì‘ìœ„ë¡œ ìƒì„±ëœ ì„ê³„ê°’ ì¤‘ ê°€ì¥ ì¢‹ì€ ê²ƒì´ ë¶„í•  ê·œì¹™ìœ¼ë¡œ ì„ íƒë¨
* ì¼ë°˜ì ìœ¼ë¡œ ë” í° í¸í–¥ ì¦ê°€ë¥¼ í¬ìƒì‹œí‚¤ë©´ì„œ ëª¨ë¸ì˜ ë¶„ì‚°ì„ ì¡°ê¸ˆ ë” ì¤„ì¼ ìˆ˜ ìˆìŒ!


### 4. Boosting

#### âœ… GBM (Gradient Boosting Machine)
* íšŒê·€ / ë¶„ë¥˜ ë¶„ì„ ìˆ˜í–‰ ê°€ëŠ¥
* ì˜ˆì¸¡ ëª¨í˜•ì˜ ì•™ìƒë¸” ë°©ë²•ë¡  ì¤‘ ë¶€ìŠ¤íŒ… ê³„ì—´ì— ì†í•¨
* ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì¤‘ì—ì„œë„ ê°€ì¥ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë†’ë‹¤ê³  ì•Œë ¤ì§
* ê³„ì‚°ëŸ‰ì´ ìƒë‹¹íˆ ë§ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— => í•˜ë“œì›¨ì–´ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬í˜„í•˜ëŠ” ê²ƒì´ ì¤‘ìš”
* Residual fitting
* êµ¬í˜„í•œ ëŒ€í‘œì  ë¼ì´ë¸ŒëŸ¬ë¦¬ : Xgboost, Catboost, LightGBM ë“±
```
1. Random Forestì™€ ë‹¤ë¥´ê²Œ ë¬´ì‘ìœ„ì„±ì´ ì—†ë‹¤.
2. ë§¤ê°œë³€ìˆ˜ë¥¼ ì˜ ì¡°ì •í•´ì•¼ í•˜ê³  í›ˆë ¨ ì‹œê°„ì´ ê¸¸ë‹¤.
3. ë°ì´í„°ì˜ scaleì— êµ¬ì• ë°›ì§€ ì•ŠëŠ”ë‹¤.
4. ê³ ì°¨ì›ì˜ í¬ì†Œí•œ ë°ì´í„°ì— ì˜ ì‘ë™í•˜ì§€ ì•ŠëŠ”ë‹¤.
```

#### âœ… XGBoost 
* Extreme Gradient Boosting
* ëª¨ë“  ê°€ëŠ¥í•œ íŠ¸ë¦¬ë¥¼ ë‚˜ì—´í•˜ì—¬ ìµœì ì˜ íŠ¸ë¦¬ë¥¼ ì°¾ëŠ” ê²ƒì´ ê±°ì˜ ë¶ˆê°€ëŠ¥
* 2ì°¨ ê·¼ì‚¬ì‹ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ì†ì‹¤í•¨ìˆ˜ë¥¼ í† ëŒ€ë¡œ ë§¤ iterationë§ˆë‹¤ í•˜ë‚˜ì˜ leafë¡œë¶€í„° ê°€ì§€ë¥¼ ëŠ˜ë ¤ë‚˜ê°€ëŠ” ê²ƒì´ íš¨ìœ¨ì 
* ì†ì‹¤í•¨ìˆ˜ê°€ ìµœëŒ€í•œ ê°ì†Œí•˜ë„ë¡ í•˜ëŠ” ë¶„í• ì ì„ ì°¾ëŠ” ê²ƒì´ ëª©í‘œ
* ë„ˆë¹„ ìš°ì„  íƒìƒ‰ì²˜ëŸ¼ ë„“ê²Œ íŠ¸ë¦¬ë¥¼ í˜•ì„±
* eval_metrics
    * rmse, mae, logloss, error, merror, mlogloss, auc, map


#### ì£¼ìš” ë§¤ê°œë³€ìˆ˜ ğŸ‘‡
  * **learning_rate** : ë†’ì„ìˆ˜ë¡ ê³¼ì í•©ë˜ê¸° ì‰¬ì›€
  * **n_estimators** : ìƒì„±í•  weaker learner ìˆ˜. learning_rateê°€ ë†’ì„ ë• n_estimatorsë¥¼ ë†’ì—¬ì•¼ ê³¼ì í•© ë°©ì§€
      * ë„ˆë¬´ ë‚®ìœ¼ë©´ underfitting
      * ë„ˆë¬´ ë†’ìœ¼ë©´ overfitting
  * **gamma** : leaf ë…¸ë“œì˜ ì¶”ê°€ ë¶„í• ì„ ê²°ì •í•  ìµœì†Œ ì†ì‹¤ ê°ì†Œê°’.
      * í•´ë‹¹ê°’ë³´ë‹¤ ì†ì‹¤ì´ í¬ê²Œ ê°ì†Œí•  ë•Œ ë¶„ë¦¬
      * ê°’ì´ ë†’ì„ìˆ˜ë¡ ê³¼ì í•© ë°©ì§€
  * **min_child_weight** : ê´€ì¸¡ì¹˜ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ í•©ì˜ ìµœì†Œ
      * ê°’ì´ ë†’ì„ìˆ˜ë¡ ê³¼ì í•© ë°©ì§€ -->

## 1. Gradient Boosting Model
* **XGBoost**
    * learning_rate ë‚®ì¶”ë©´ n_estimators ë†’ì—¬ì£¼ê¸°
    * gamma ë†’ì—¬ì„œ ê³¼ì í•© ë°©ì§€
    * xgb tree ê·¸ë¦¬ê¸°
        * xgb.plot_tree
    * xgb í”¼ì²˜ ì¤‘ìš”ë„ ê·¸ë¦¬ê¸°
        * xgb.plot_importance
* **lightGBM**
    * boosting type
        * goss / efb
    * lgbm tree ê·¸ë¦¬ê¸°
        * lgbm.plot_tree
            * show_info : split_gain, interval_value, internal_count, leaf_count
    * lgbm í”¼ì²˜ ì¤‘ìš”ë„ ê·¸ë¦¬ê¸°
        * lgbm.plot_importance
* **CatBoost**
    * grow_policy
        * SymmetricTree - ëŒ€ì¹­íŠ¸ë¦¬
        * Lossguide - ë¦¬í”„ë³„
        * Depthwise - ê¹Šì´ë³„
    * randomized_research
        * ì£¼ì–´ì§„ paraterì—ì„œ best parameter ì¡°í•© ì°¾ì•„ë‚´ê¸°
        * í•´ë‹¹ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì„œ ì‹œê°í™”í•˜ê¸°
        * `pd.DataFrame({"train-R2-mean": df_result.loc["train-R2-mean"], "test-R2-mean" :  df_result.loc["test-R2-mean"] }).plot()`
    * ğŸ¤” cat_featuresë¥¼ ì§€ì •í•´ì„œ ì‹¤ìŠµí•˜ëŠ” ì´ìœ ?
        * ì „ì²˜ë¦¬ / ì¸ì½”ë”© ì—†ì´ ì‰½ê²Œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ì„œ

## 2. ë²”ì£¼í˜• ë°ì´í„° ë‹¤ë£¨ê¸°
ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬ í•˜ì§€ ì•Šê³  category typeìœ¼ë¡œ data typeë§Œ ë°”ê¿”ì£¼ë©´ ì•Œì•„ì„œ í•™ìŠµí•œë‹¤.

```python
train[cat_col] = train[cat_col].astype("category")
test[cat_col] = test[cat_col].astype("category")
```



## 3. ë¶ˆê· í˜• ë°ì´í„°
* ìƒì‚°ê³¼ì • ì–‘ë¶ˆì—¬ë¶€
    * 95% í™•ë¥ ë¡œ ì •ìƒ ì œí’ˆì„, 5% í™•ë¥ ë¡œ ë¶ˆëŸ‰í’ˆì„ ë§Œë“¤ì–´ë‚´ëŠ” ê³µì¥
* ì•”í™˜ì ì—¬ë¶€
* ì€í–‰ëŒ€ì¶œì—¬ë¶€ íŒë‹¨
* ì‹ ìš©ì¹´ë“œ ì‚¬ê¸°
* ê²Œì„, ê´‘ê³  ì–´ë·°ì§•
* ì§€ì§„, í•´ì¼, ì¬ë‚œ ì˜ˆì¸¡

## 4. Confusion Matrix (í˜¼ë™ í–‰ë ¬)

![](/assets/img/img_221124/confusion_matrix.png){: .center width="60%"}
- **TN(True Negative, Negative Negative)**: ì‹¤ì œëŠ” Negativeì¸ë°, Negativeë¡œ ì˜ˆì¸¡í•¨.
    - Negativeê°€ ë§ì•„! â‡’  ì•„ë‹ŒÂ ê²ƒì„ ì˜¬ë°”ë¥´ê²Œ í‹€ë¦¬ë‹¤ê³  ì˜ˆì¸¡í•¨
- **FP(False Positive, Negative Positive)**: ì‹¤ì œëŠ” Negativeì¸ë°, Positiveë¡œ ì˜ˆì¸¡í•¨.
    - Positiveê°€ ì•„ë‹ˆì•¼! â‡’ ì•„ë‹ŒÂ ê²ƒì„ ì˜¬ë°”ë¥´ì§€ ì•Šê²Œ ë§ë‹¤ê³  ì˜ˆì¸¡í•¨
- **FN(False Negative, Positive Negative)**: ì‹¤ì œëŠ” Positiveì¸ë°, Negativeë¡œ ì˜ˆì¸¡í•¨.
    - Negativeê°€ ì•„ë‹ˆì•¼! â‡’ ë§ëŠ” ê²ƒì„ ì˜¬ë°”ë¥´ì§€ ì•Šê²Œ í‹€ë¦¬ë‹¤ê³  ì˜ˆì¸¡í•¨
- **TP(True Positive, Positive Positive)**: ì‹¤ì œëŠ” Positiveì¸ë°, Positiveë¡œ ì˜ˆì¸¡í•¨.
    - Positiveê°€ ë§ì•„! â‡’ ë§ëŠ” ê²ƒì„ ì˜¬ë°”ë¥´ê²Œ ë§ë‹¤ê³  ì˜ˆì¸¡í•¨

## 5. ë¶„ë¥˜ ëª¨ë¸ì˜ í‰ê°€ ë°©ë²• : Accuracy

### Accuracy
- ì˜ˆì¸¡ ê²°ê³¼ ì „ì²´ ê±´ìˆ˜ ì¤‘ ì‹¤ì œ ê°’ì„ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚¸ë‹¤.
- ë”°ë¼ì„œ ì •í™•ë„ëŠ” 0ì—ì„œ 1ì‚¬ì´ì˜ ê°’ì„ ê°€ì§„ë‹¤.
- ë‹¤ìŒ ì½”ë“œë¥¼ ì´ìš©í•´ ì •í™•ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŒ
```python
from sklearn.metrics import accuracy_score
accuracy_score(df.y, pred)
```

### Example

|ì‹¤ì œê°’ / ì˜ˆì¸¡ê°’|ì¼ë°˜í™˜ì|ì•”í™˜ì|
|:---:|:---:|:---:|
|ì¼ë°˜í™˜ì|90|1|
|ì•”í™˜ì|7|2|

ì•”í™˜ìë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¶„ë¥˜ ì‹œìŠ¤í…œì´ ìˆë‹¤ê³  í•˜ì. í•´ë‹¹ ë¶„ë¥˜ê¸°ë¡œ **100ëª…**ì˜ í™˜ìì˜ ì•”ì—¬ë¶€ë¥¼ íŒë‹¨í–ˆë‹¤ê³  í–ˆì„ ë•Œì˜ ê²°ê³¼ê°€ ìœ„ì™€ ê°™ì„ ë•Œ ì •í™•ë„ëŠ” ì–¼ë§ˆë‚˜ ë ê¹Œ?

â¡ï¸ **ì •í™•ë„**ëŠ” ( 2 + 90 ) / ( 2 + 7 + 1 + 90 ) X  100 = **<font color='darkred'>92 (%)</font>**

92 %ì˜ ì •í™•ë„ë¡œ êµ‰ì¥íˆ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì—¬ì£¼ì§€ë§Œ,
**ì•”í™˜ì**ë§Œ ë¶„ë¦¬í•´ì„œ ìƒê°í•˜ë©´ 2 + 7 = **9 (ëª…)** ì˜ ì•”í™˜ì ì¤‘ **2ëª…**ë§Œ ë§ê²Œ ì˜ˆì¸¡í•œ ê²ƒì´ë¯€ë¡œ ëª¨ë¸ ì „ì²´ì˜ ì •í™•ë„ëŠ” ë†’ì§€ë§Œ **<mark style='background-color: #fff5b1'>ì •ì‘ í•„ìš”í–ˆë˜ ì•”í™˜ì ì˜ˆì¸¡ ì„±ëŠ¥</mark>**ì€ 2 / 9 X 100 = **<font color='red'>ì•½ 22 (%)</font>**ë¡œ ë§¤ìš° ë‚®ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.

#### â¡ï¸ ë‹¤ë¥¸ í‰ê°€ ë°©ë²•ì´ í•„ìš”í•˜ë‹¤ !




## 6. ì œì•ˆëœ ë‹¤ë¥¸ í‰ê°€ ë°©ë²•
### âœ… Precision
![](/assets/img/img_221124/precision.png){: .center width="40%"}

* ì •ë°€ë„
* **<mark style='background-color: #fff5b1'>ì˜ˆì¸¡ê°’</mark>** ì¤‘ì— ì–¼ë§ˆë‚˜ ë§ì•˜ëŠ”ì§€
  * **ì˜ˆì¸¡ì„ Positiveë¡œ í•œ ëŒ€ìƒ ì¤‘ ì˜ˆì¸¡ê°’**ê³¼ **ì‹¤ì œê°’ì´ Positiveë¡œ ì¼ì¹˜**í•œ ë°ì´í„°ì˜ ë¹„ìœ¨
* **Positive ì˜ˆì¸¡ ì„±ëŠ¥**ì„ ë”ìš± ì •ë°€í•˜ê²Œ ì¸¡ì •í•˜ê¸° ìœ„í•œ í‰ê°€ ì§€í‘œ <br/>
 â¡ï¸ ì •ë°€ë„ëŠ” FPë¥¼ ë‚®ì¶”ëŠ” ë° ì´ˆì ì„ ë§ì¶¤
* 1ì¢… ì˜¤ë¥˜
  * ì‹¤ì œ Negative ë°ì´í„° ì˜ˆì¸¡ì„ Positive ì–‘ì„±ìœ¼ë¡œ ì˜ëª» íŒë‹¨í•˜ê²Œ ë˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ë°œìƒí•˜ëŠ” ê²½ìš°
```python
from sklearn.metrics import precision_score
precision_score(y_test, y_pred)
```


### âœ… Recall
![](/assets/img/img_221124/recall.png){: .center width="40%"}

* ì¬í˜„ë„
* **<mark style='background-color: #fff5b1'>ì‹¤ì œê°’</mark>** ì¤‘ì— ì–¼ë§ˆë‚˜ ë§ì•˜ëŠ”ì§€
  * ì¬í˜„ë„(ë¯¼ê°ë„)ëŠ” í˜„ì‹¤ì—ì„œ Negativeì¼ ë•Œ ì˜ˆì¸¡ì´ ì–´ë–»ê²Œ ì´ë£¨ì–´ì§€ëŠ”ì§€ì— ëŒ€í•œ ì •ë³´ëŠ” ì œê³µí•˜ì§€ ì•ŠìŒ.
* 2ì¢… ì˜¤ë¥˜
```python
from sklearn.metrics import recall_score
recall_score(df.y, pred)
```


### âœ… F1 Score

* **1ì¢… ì˜¤ë¥˜ <font color='lightgray'>FP</font>**
    * ì‹¤ì œëŠ” Negativeì¸ë° Positiveë¡œ ì˜ˆì¸¡
    * ì„ì‹ ì´ ì•„ë‹Œë°, ì„ì‹ ìœ¼ë¡œ ì˜ˆì¸¡
    * ë¬´ê³ í•œ í”¼ê³ ì¸ì—ê²Œ ìœ ì£„ ì„ ê³ 
    * ìŠ¤íŒ¸ë©”ì¼ì´ ì•„ë‹Œë° ìŠ¤íŒ¸ë©”ì¼ë¡œ ì˜ˆì¸¡
* **2ì¢… ì˜¤ë¥˜ <font color='lightgray'>FN</font>**
    * ì‹¤ì œëŠ” Positiveì¸ë° Negativeë¡œ ì˜ˆì¸¡
    * ì„ì‹ ì¸ë°, ì„ì‹ ì´ ì•„ë‹Œ ê²ƒìœ¼ë¡œ ì˜ˆì¸¡
    * í™”ì¬ê°€ ë‚¬ëŠ”ë° í™”ì¬ê°€ ì•„ë‹ˆë¼ê³  ì˜ˆì¸¡

## ğŸ’¡ ì˜ˆí”„ë¦¬, ì‹¤ë¦¬ì½œ
ì´ë ‡ê²Œ ì™¸ìš°ë©´ precisionê³¼ recallì„ ëœ í—·ê°ˆë¦¬ê²Œ ì™¸ìš¸ ìˆ˜ ìˆë‹¤!





<br/>

***

## ğŸ’» ì‹¤ìŠµ ì˜ˆì œ ì½”ë“œ
### : ğŸ› Benz Boosting Model

#### 1. XGBoost

```python
# xgboostëŠ” gradient boosting tree(GBT)ì˜ ë³‘ë ¬ í•™ìŠµì„ êµ¬í˜„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
import xgboost as xgb

model_xgb = xgb.XGBRegressor(random_state = 42, n_jobs = -1,
                             learning_rate = 0.05, n_estimators = 200, gamma = 10)
# fit
model_xgb.fit(X_train, y_train)
```

```python
xgb.plot_importance(model_xgb);
```

```python
xgb.plot_tree(model_xgb, num_trees=1)
fig = plt.gcf()
fig.set_size_inches(30, 20)
```
![](/assets/img/img_221124/xgb.png){: .center width="80%"}


```python
score_xgb = model_xgb.score(X_valid, y_valid)
score_xgb = round(score_xgb, 5)
```

```python
y_pred_xgb = model_xgb.predict(X_test)
y_pred_xgb
```



#### 2. Light GBM

```python
# lightgbm 
import lightgbm as lgbm
model_lgbm = lgbm.LGBMRegressor(random_state = 42, n_jobs = -1,
                                learning_rate = 0.01, n_estimators = 400)
# fit
model_lgbm.fit(X_train, y_train)
```

```python
lgbm.plot_importance(model_lgbm)
```

```python
lgbm.plot_tree(model_lgbm, figsize=(20, 20), tree_index=0, 
               show_info=['split_gain', 'internal_value', 'internal_count', 'leaf_count'])
```
![](/assets/img/img_221124/lgbm.png){: .center width="90%"}

```python
score_lgbm = model_lgbm.score(X_valid, y_valid)
score_lgbm = round(score_lgbm, 5)
```

```python
# predict
y_pred_lgbm = model_lgbm.predict(X_test)
y_pred_lgbm
```

#### 3. CatBoost

```python
# catboost
# eval_metric='R2'
from catboost import CatBoostRegressor
model_cat = CatBoostRegressor(
            eval_metric = 'R2',
            verbose = False,
            random_state=42
    )
```
```python
# SymmetricTree - ëŒ€ì¹­íŠ¸ë¦¬
# Lossguide - ë¦¬í”„ë³„ 
# Depthwise - ê¹Šì´ë³„
from scipy.stats import randint
from sklearn.utils.fixes import loguniform

param_grid = {
    'n_estimators': randint(100, 300),
    'depth': randint(1, 5),
    'learning_rate': loguniform(1e-3, 0.1),
    'min_child_samples': randint(10, 40),
    'grow_policy': ['SymmetricTree', 'Lossguide', 'Depthwise']
}
```
```python
# randomized_search
result = model_cat.randomized_search(param_grid, X_train, y_train, cv=3, n_iter=10)
result
```
```python
df_result = pd.DataFrame(result)
df_result = df_result.loc[["train-R2-mean", "test-R2-mean"], "cv_results"]
df_result
```
```python
pd.DataFrame({"train-R2-mean": df_result.loc["train-R2-mean"], 
              "test-R2-mean" :  df_result.loc["test-R2-mean"] }).tail(3)
```
```python
pd.DataFrame({"train-R2-mean": df_result.loc["train-R2-mean"]
              "test-R2-mean" :  df_result.loc["test-R2-mean"] }).plot()
```
![](/assets/img/img_221124/r2_mean_all.png){: .center width="60%"}

```python
pd.DataFrame({"train-R2-mean": df_result.loc["train-R2-mean"], 
              "test-R2-mean" :  df_result.loc["test-R2-mean"] }).tail(50).plot()
```
![](/assets/img/img_221124/r2_mean_tail.png){: .center width="60%"}

```python
# fit
model_cat.fit(X_train, y_train)

# valid score
score_cat = model_cat.score(X_valid, y_valid)
score_cat = round(score_cat, 5)

# predict
y_pred = model_cat.predict(X_test)
```


### ë‹¤ìŒ í¬ìŠ¤íŠ¸ì—ì„œ ë§Œë‚˜ìš” ğŸ™Œ


<br/>

***

## ì°¸ê³  <br/>

### ğŸ¤” learning rateì™€ n_estimator?
learning_rateë¥¼ ì¤„ì¸ë‹¤ë©´ ê°€ì¤‘ì¹˜ ê°±ì‹ ì˜ ë³€ë™í­ì´ ê°ì†Œí•´ì„œ, ì—¬ëŸ¬ í•™ìŠµê¸°ë“¤ì˜ ê²°ì • ê²½ê³„(decision boundary) ì°¨ì´ê°€ ì¤„ì–´ë“¤ê²Œ ëœë‹¤.

n_estimators ë¥¼ ëŠ˜ë¦°ë‹¤ë©´ ìƒì„±í•˜ëŠ” ì•½í•œ ëª¨ë¸(weak learner, tree)ê°€ ëŠ˜ì–´ë‚˜ê²Œ ë˜ê³ , ì•½í•œ ëª¨ë¸ì´ ë§ì•„ì§„ë§Œí¼ ê²°ì • ê²½ê³„(decision boundary)ê°€ ë§ì•„ì§€ë©´ì„œ ëª¨ë¸ì´ ë³µì¡í•´ì§€ê²Œ ëœë‹¤. 

ì¦‰, ë¶€ìŠ¤íŒ…ì•Œê³ ë¦¬ì¦˜ì—ì„œ n_estimatorsì™€ learning_rateëŠ” trade-off ê´€ê³„ì…ë‹ˆë‹¤. n_estimators(ë˜ëŠ” learning_rate)ë¥¼ ëŠ˜ë¦¬ê³ , learning_rate(ë˜ëŠ” n_estimators)ì„ ì¤„ì¸ë‹¤ë©´ ì„œë¡œ íš¨ê³¼ê°€ ìƒì‡„ëœë‹¤.

### ğŸ¤” Boosting ëª¨ë¸ ì‹œê°í™”
ë°°ê¹… ëª¨ë¸ì€ ì‹œê°í™”ê°€ ì–´ë ¤ì›Œ 3rd party ë„êµ¬ë¥¼ ë”°ë¡œ ì„¤ì¹˜í•´ì•¼ ì‹œê°í™” ê°€ëŠ¥í•©ë‹ˆë‹¤. ê·¸ê²ƒë„ ê°œë³„ íŠ¸ë¦¬ë¥¼ ì‹œê°í™” í•˜ëŠ” ê²ƒì€ ì–´ë µìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ë¶€ìŠ¤íŒ… ëª¨ë¸ì€ ì™œ ì‹œê°í™”ê°€ ê°€ëŠ¥í• ê¹Œìš”?

=> ë°°ê¹…ëª¨ë¸ì€ ë³‘ë ¬ì ìœ¼ë¡œ íŠ¸ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œ ìƒì„±í•©ë‹ˆë‹¤. ê·¸ëŸ°ë° ë¶€ìŠ¤íŒ…ì€ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.


### ğŸ¤” ë¶€ìŠ¤íŒ… ëª¨ë¸ì€ ì™œ overfittingì— ë¯¼ê°í• ê¹Œìš”?
ì´ì „ íŠ¸ë¦¬ (ì´ì „ í•™ìŠµ) ê°€ ë‹¤ìŒ íŠ¸ë¦¬ (ë‹¤ìŒ í•™ìŠµ) ì— ì˜í–¥ì„ ì£¼ê¸° ë•Œë¬¸ì´ë‹¤.

### â—ï¸ n_jobs
ê°€ë” ì„±ëŠ¥ì´ ë‚®ì€ ì¥ë¹„ì—ì„œ n_jobs=-1ë¡œ ì‚¬ìš©í•˜ë©´ ë…¸íŠ¸ë¶ì´ dead kernelì´ ë˜ëŠ” í˜„ìƒì´ ë‚˜íƒ€ë‚œë‹¤. ì„±ëŠ¥ì´ ë‚®ì€ ì¥ë¹„ì´ê±°ë‚˜ ë‹¤ë¥¸ ì‘ì—…ì„ ë§ì´ ì§„í–‰í•˜ê³  ìˆë‹¤ë©´ n_jobs = 1ë¡œ ì„¤ì •í•˜ë©´ ì¢€ ë‚«ë‹¤.

### ğŸ”¥ ë°ì´í„° ë¶„ì„ê°€ì—ê²Œ MLê¸°ìˆ ì§ˆë¬¸ìœ¼ë¡œ ìì£¼ ë“±ì¥í•˜ëŠ” ì§ˆë¬¸
- Cross Validationì€ ë¬´ì—‡ì´ê³  ì–´ë–»ê²Œ í•´ì•¼í•˜ë‚˜ìš”?
- íšŒê·€ / ë¶„ë¥˜ì‹œ ì•Œë§ì€ metricì€ ë¬´ì—‡ì¼ê¹Œìš”?
- ì•Œê³  ìˆëŠ” metricì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”. (ex. RMSE, MAE, recall, precision â€¦)
- ì •ê·œí™”ë¥¼ ì™œ í•´ì•¼í• ê¹Œìš”? ì •ê·œí™”ì˜ ë°©ë²•ì€ ë¬´ì—‡ì´ ìˆë‚˜ìš”?
- Local Minimaì™€ Global Minimaì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.
- ì°¨ì›ì˜ ì €ì£¼ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”
- dimension reductionê¸°ë²•ìœ¼ë¡œ ë³´í†µ ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?


### ğŸ¤” confusion matrixì˜ ê¸°ì¤€?
confusion matrix ê°€ ì±…ë§ˆë‹¤ ë¸”ë¡œê·¸ë§ˆë‹¤ ìœ„í‚¤ë§ˆë‹¤ ì‚¬ì´íŠ¸ë§ˆë‹¤ ìˆœì„œ ê°€ ë‹¤ ë‹¤ë¦…ë‹ˆë‹¤. ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ ë´ì•¼ í• ê¹Œìš”?

-> ì‚¬ì´í‚·ëŸ° ê¸°ì¤€ìœ¼ë¡œ ë³´ëŠ” ê²Œ ëœ í˜¼ë€ìŠ¤ëŸ½ë‹¤.

### ìë£Œ

* **[ë©‹ì‚¬ 7ê¸° ìœ¼ìŒ°ìœ¼ìŒ° ë³µìŠµ] 5ï¸âƒ£ ë§ˆì´ê°“ì˜ìš´ 5ï¸âƒ£ íˆë ¤ì¢‹ì•„**



<!-- ### ğŸ¾ã€€ã€€ğŸ¾
### ğŸ¾ã€€ã€€ğŸ¾
### ğŸ¾ã€€ã€€ğŸ¾
### ğŸ¾ã€€ã€€ğŸ¾
### ğŸ¾ã€€ã€€ğŸ¾
### ğŸ¾ã€€ã€€ğŸ¾ 
<font color='dodgerblue'> ì˜ˆìœ íŒŒë‘ </font>
<font color='lightgray'>Miss</font>
<mark style='background-color: #f1f8ff'> ì—°í•œ íŒŒë‘ </mark>
<mark style='background-color: #fff5b1'> ì—°í•œ ë…¸ë‘ </mark>
<mark style='background-color: #ffdce0'> ì—°í•œ ë¹¨ê°• </mark>
<mark style='background-color: #dcffe4'> ì—°í•œ ì´ˆë¡ </mark>
<mark style='background-color: #f5f0ff'> ì—°í•œ ë³´ë¼ </mark>
<mark style='background-color: #f6f8fa'> ì—°í•œ íšŒìƒ‰ </mark>
-->
